{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Courts and Tribunals Judiciary Website to collect Prevent Future Death (PFD) reports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "from time import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "    \n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "import concurrent\n",
    "\n",
    "from time import sleep, time\n",
    "\n",
    "def get_url(url):\n",
    "    return BeautifulSoup(get(url, verify = False).content, \"html.parser\")\n",
    "\n",
    "def retries(record_url, tries=3):\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            soup = get_url(record_url)\n",
    "            return soup\n",
    "        except (ConnectionError, SSLError):\n",
    "            sleep(2)\n",
    "            continue\n",
    "    raise ConnectionError(\"Connection error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper starts here - last run on Monday, 06 Sept 2021, 9.06 am."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find the number of pages containing PFD reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"https://www.judiciary.uk/subject/prevention-of-future-deaths/\"\n",
    "number_of_pages = int(get_url(prefix) \\\n",
    "                    .find(\"div\", \"pagination\") \\\n",
    "                    .find_all(\"li\")[-1] \\\n",
    "                    .find(\"a\")[\"href\"] \\\n",
    "                    .split(\"/\")[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_urls(page):\n",
    "    soup = retries(prefix+\"/page/{}\".format(str(page+1)))\n",
    "    h5s = soup.find_all('h5', {'class': 'entry-title'})\n",
    "    return [h5.a.get('href') for h5 in h5s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18993f70f6d4d0eaf938c9bf7673738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "record_urls = []\n",
    "with tqdm(total = number_of_pages) as pbar:\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=16) as executor:\n",
    "        record_urls = [executor.submit(fetch_urls, i)\n",
    "                       for i in range(number_of_pages)]\n",
    "        for future in concurrent.futures.as_completed(record_urls):\n",
    "            pbar.update(1)\n",
    "record_urls = [link for links in [urls.result() for urls in record_urls] for link in links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check how many records (i.e. cases) were pulled from the urls & the first and last case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "class MissingRecordsError(Exception): pass\n",
    "class MissingFieldError(Exception): pass\n",
    "class UnreadableFieldError(Exception): pass\n",
    "class SpecialCaseUnaccountedForError(Exception): pass\n",
    "\n",
    "special_cases = [\n",
    "    \"https://www.judiciary.uk/publications/roadsafety/\",\n",
    "    \"https://www.judiciary.uk/publications/helen-sheath/\",\n",
    "    \"https://www.judiciary.uk/publications/rebecca-evans/\"\n",
    "]\n",
    "\n",
    "columns = list(map(lambda x: x.lower(), [\n",
    "    'Date of report',\n",
    "    'Ref',\n",
    "    'Deceased name',\n",
    "    'Coroner name',\n",
    "    'Coroner Area',\n",
    "    'Coroners Area', #NB to be merged later; if both come up there'll be trouble\n",
    "    'Category',\n",
    "    \"This report is being sent to\"\n",
    "]))\n",
    "\n",
    "records, refs, pdflinks = [], [], []\n",
    "\n",
    "plurals_possessives = re.compile(r\"â€™s\\s|s\\s|'s\\s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class page_scrape(object):\n",
    "    def __init__(self, url):       \n",
    "#         try:\n",
    "        self.url = url\n",
    "        self.soup = retries(url, tries=5) #raises connexion error if fails\n",
    "        self.information = self.soup.find(\"div\", {\"class\": \"entry-content\"}).find_all(\"p\")\n",
    "\n",
    "        # nothing between <p> to process\n",
    "        if not self.information:\n",
    "            raise MissingRecordsError         \n",
    "\n",
    "        self.extracted = {\"url\": url}\n",
    "\n",
    "        if url in special_cases:\n",
    "            self._process_special_case(url)\n",
    "        else:\n",
    "             for field in self.information:\n",
    "                extraction = self.strip_field(field)\n",
    "                self.extracted[extraction.header] = extraction.text\n",
    "\n",
    "        download_box = self.soup.find(\"div\", \"download-box\")\n",
    "\n",
    "        links = download_box.find_all('a', href=True)\n",
    "        responses = len([s for s in links if \"response\" in s.text.lower()])\n",
    "\n",
    "        pdflinks.append([link.href for link in links])\n",
    "\n",
    "        self.extracted[\"number of links\"] = len(links)\n",
    "        self.extracted[\"number of responses\"] = responses\n",
    "\n",
    "        try:\n",
    "            self.extracted[\"number of recipients\"] = self.extracted[\"this report is being sent to\"].count(\";\") + 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if self.extracted[\"ref\"] in refs: # deduplication of references\n",
    "                self.extracted[\"ref\"] += \"-bis\"\n",
    "            refs.append(self.extracted[\"ref\"])\n",
    "        except KeyError:\n",
    "            self.extracted[\"ref\"] = \"\" # no reference found\n",
    "\n",
    "#         except Exception as e:\n",
    "#             details = str(e)\n",
    "\n",
    "#             if isinstance(e, ConnectionError):\n",
    "#                 details = \"Could not connect\"\n",
    "\n",
    "#             if isinstance(e, MissingRecordsError):\n",
    "#                 details = \"No records found\"\n",
    "\n",
    "#             if isinstance(e, SpecialCaseUnaccountedForError):\n",
    "#                 details = \"Special case unaccounted for\"\n",
    "\n",
    "#             else:\n",
    "#                 raise e\n",
    "\n",
    "            #errors.append({\"url\": url, \"reason\": details})\n",
    "            #print(details)\n",
    "        \n",
    "    def _process_special_case(self,url):\n",
    "        if self.url == 'https://www.judiciary.uk/publications/roadsafety/':\n",
    "            fields = self.information[0].find_all('strong')\n",
    "            heads = ['date of report',\n",
    "                     'ref',\n",
    "                     'deceased name',\n",
    "                     'coroner name',\n",
    "                     'coroner area',\n",
    "                     'category']\n",
    "            for field, h in zip(fields,heads):\n",
    "                self.extracted[h] = field.next_sibling.replace(':','').replace('Ref','').strip()\n",
    "        elif url == 'https://www.judiciary.uk/publications/helen-sheath/':\n",
    "            fields = self.information[0].text.split('\\n')\n",
    "            vals = [fields.split(\":\") for field in fields]\n",
    "            for v in vals:\n",
    "                if v[0] == \"Coroners name\":\n",
    "                    alt = \"coroner_name\"\n",
    "                elif v[0] == \"Coroners Area\":\n",
    "                    alt = \"coroner_area\"\n",
    "                else:\n",
    "                    alt = v[0].strip().replace(' ','_').lower()\n",
    "                self.extracted[alt] = v[1].strip().replace('\\n','')\n",
    "        elif url == \"https://www.judiciary.uk/publications/rebecca-evans/\":\n",
    "            for field in self.information:\n",
    "                if \"Rebecca-EvansR\" in field.text:\n",
    "                    self.extracted[\"category\"] = field.text.split(':')[1].strip().replace('\\n','')\n",
    "                else:\n",
    "                    strip_field(field)\n",
    "\n",
    "        else:\n",
    "            raise SpecialCaseUnaccountedForError\n",
    "\n",
    "            \n",
    "            \n",
    "    class strip_field(object):\n",
    "        def __init__(self, field):\n",
    "            #try:\n",
    "            self.field_text = field.text.strip()\n",
    "\n",
    "            if self.field_text == \"\": return\n",
    "\n",
    "            self.pre_colon, self.post_colon = self.field_text.split(\":\", 1) # split by first colon\n",
    "            self.header = self.pre_colon.lower()\n",
    "            \n",
    "#             print(self.field_text)\n",
    "#             print(self.pre_colon)\n",
    "#             print(self.post_colon)\n",
    "#             print(self.header)\n",
    "            \n",
    "            if self.header in columns: # Normal case: colon separates text\n",
    "                self.text = self.post_colon.strip().replace('\\n','').replace('\\xa0','')\n",
    "#                 print(\"Normal case\")\n",
    "#                 print(self.header)\n",
    "#                 print(self.text)\n",
    "\n",
    "            elif self.post_colon.strip() == \"\": # no colon separator\n",
    "                self.header, self.text = self._no_colon_separator()          \n",
    "#                 print(\"No colon case\")\n",
    "#                 print(self.header)\n",
    "#                 print(self.text)\n",
    "            elif self.field_text.count(\":\") == 2: # two colons\n",
    "                self.header, self.text = self._two_colons()\n",
    "#                 print(\"Two colons case\")\n",
    "#                 print(self.header)\n",
    "#                 print(self.text)\n",
    "            elif ':' in self.field_text and self.header not in columns: # plurals and possessives\n",
    "                self.header, self.text = self._plurals_and_possessives()\n",
    "#                 print(\"Plurals and possessives\")\n",
    "#                 print(self.header)\n",
    "#                 print(self.text)\n",
    "            \n",
    "\n",
    "#             except Exception as e:\n",
    "#                 details = str(e)\n",
    "\n",
    "#                 if isinstance(e, UnreadableFieldError):\n",
    "#                     details = f\"Unreadable field; text read: {field.text}\"\n",
    "#                     errors.append({\"url\": url, \"reason\": reason})\n",
    "\n",
    "#                 else:\n",
    "#                     raise e\n",
    "#                 print(details)\n",
    "\n",
    "        def _no_colon_separator(self):\n",
    "            column_found = False\n",
    "            for column in columns:\n",
    "                if self.pre_colon.startswith(column):\n",
    "                    header = column\n",
    "                    text = pre_colon[len(column)+1] \\\n",
    "                            .replace('\\n','') \\\n",
    "                            .replace('\\xa0','')\n",
    "                    column_found = True\n",
    "                    break\n",
    "            if not column_found:\n",
    "                raise UnreadableFieldError\n",
    "            return (header, text)\n",
    "\n",
    "        def _two_colons(self):\n",
    "            split_by_colon = self.field_text.split(':')\n",
    "            header = re.sub(plurals_possessives,\n",
    "                             ' ',\n",
    "                             split_by_colon[0]+split_by_colon[1]\n",
    "                            ) \\\n",
    "                            .strip() \\\n",
    "                            .lower()\n",
    "            text = split_by_colon[2] \\\n",
    "                        .strip() \\\n",
    "                        .replace('\\n','') \\\n",
    "                        .replace('\\xa0','')\n",
    "            return (header, text)\n",
    "\n",
    "        def _plurals_and_possessives(self):\n",
    "            if 'Name of' in self.field_text:\n",
    "                header = self.pre_colon.split(' ')[2] + \" name\"\n",
    "                text = self.field_text[-1].strip().replace('\\n','').replace('\\xa0','')\n",
    "            else:     \n",
    "                header = re.sub(plurals_possessives, ' ', self.field_text[0]).strip()\n",
    "                text = self.field_text[-1].strip().replace('\\n','').replace('\\xa0','')\n",
    "            return (header, text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clear old errors\n",
    "errors = []\n",
    "\n",
    "with tqdm(total = 100) as pbar:\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=16) as executor:\n",
    "        records = [executor.submit(scrape_from_page, url) for url in record_urls[:100]]\n",
    "        for future in concurrent.futures.as_completed(records):\n",
    "            pbar.update(1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'field_text' is not defined\n",
      "name 'field_text' is not defined\n",
      "name 'field_text' is not defined\n",
      "name 'field_text' is not defined\n",
      "name 'field_text' is not defined\n",
      "name 'field_text' is not defined\n",
      "name 'field_text' is not defined\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() should return None, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-253bad06c586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpage_scrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_urls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() should return None, not 'dict'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = re.compile(r\"â€™s\\s|s\\s|'s\\s\")\n",
    "text_cats = ['Date of report', 'Ref', 'Deceased name', 'Coroner name', 'Coroner Area', 'Category', \"This report is being sent to\"]\n",
    "#First, I create two lists, one for the PDFs and one for the text data\n",
    "record_text = []\n",
    "pdf_urls = []\n",
    "ref_list = []\n",
    "\n",
    "errors = []\n",
    "\n",
    "record_count = 0\n",
    "for record_url in tqdm(record_urls[:5]):\n",
    "    try:\n",
    "    \n",
    "        try:\n",
    "            soup = retries(record_url, tries=5)\n",
    "        \n",
    "        except ConnectionError:\n",
    "            print(f\"{record_url} could not connect\")\n",
    "            errors.append(error_details(error_dict, record_count, record_url, 'Connection Error'))\n",
    "            record_count +=1\n",
    "            continue\n",
    "\n",
    "        #This gets all the text fields from the website to work with\n",
    "        death_info = soup.find('div', {'class':'entry-content'}).find_all('p')\n",
    "        \n",
    "        if not death_info:\n",
    "            print(f\"{record_url} produced no data\")\n",
    "            error_catching.append(error_details(error_dict, record_count, record_url, 'No Text Loaded'))\n",
    "            record_count +=1\n",
    "            continue\n",
    "            \n",
    "        #Our dictionary that will hold all of the text information that we will eventually append to \"record_text\"\n",
    "        blankdict = {}\n",
    "        \n",
    "        #This is to handle 1 annoying record with messed up html tags\n",
    "        if record_url == 'https://www.judiciary.uk/publications/roadsafety/':\n",
    "            strong = death_info[0].find_all('strong')\n",
    "            heads = ['date_of_report', 'ref', 'deceased_name', 'coroner_name', 'coroner_area', 'category']\n",
    "            for st, h in zip(strong,heads):\n",
    "                blankdict[h] = st.next_sibling.replace(':','').replace('Ref','').strip()\n",
    "        #And another record with wonky html\n",
    "        elif record_url == 'https://www.judiciary.uk/publications/helen-sheath/':\n",
    "            brs = death_info[0].text.split('\\n')\n",
    "            vals = []\n",
    "            for b in brs:\n",
    "                vals.append(b.split(':'))\n",
    "            for v in vals:\n",
    "                if v[0] == \"Coroners name\":\n",
    "                    alt = \"coroner_name\"\n",
    "                    blankdict[alt] = v[1].strip().replace('\\n','')\n",
    "                elif v[0] == \"Coroners Area\":\n",
    "                    alt = \"coroner_area\"\n",
    "                    blankdict[alt] = v[1].strip().replace('\\n','')\n",
    "                else:\n",
    "                    blankdict[v[0].strip().replace(' ','_').lower()] = v[1].strip().replace('\\n','')\n",
    "        else:        \n",
    "            #looping through all of the text categories for handling\n",
    "            for p in death_info:\n",
    "                #This checks for blank fields and if there is nothing, it skips it\n",
    "                if p.text.strip() == '':\n",
    "                    pass\n",
    "                #This checks for our \"Normal\" case in which a colon exists and the category is one of the ones we \n",
    "                #pre-specified above in the \"text_cats\" list\n",
    "                #We also need to account here for one strange record for \"Rebecca Evans\" which has a weird text error\n",
    "                #That we manually correct for\n",
    "                elif ':' in p.text and p.text.split(':')[0] in text_cats and not 'Rebecca-EvansR.pdf' in p.text:\n",
    "                    #Simply assigning the key and value from strings on either side of the colon, making everything \n",
    "                    #lower case and replacing spaces with underscores and also removing any stray semi-colons\n",
    "                    text_list = p.text.split(':')\n",
    "                    blankdict[text_list[0].strip().replace(' ','_').lower()] = text_list[1].strip().replace('\\n','').replace('\\xa0','')\n",
    "\n",
    "                elif 'Rebecca-EvansR.pdf' in p.text:\n",
    "                    #This deals with that singular odd record that currently exists as of 8 Nov 2019\n",
    "                    blankdict['category'] = p.text.split(':')[1].strip().replace('\\n','')\n",
    "                    \n",
    "                elif ':' not in p.text:\n",
    "                    #If the string doesn't have a colon, we can't split on it so have to get it into dictionary format\n",
    "                    #Using an alternate method that counts the length of the thing\n",
    "                    if any(x in p.text for x in text_cats):\n",
    "                        t = [x for x in text_cats if x in p.text][0]\n",
    "                        l = len(t)\n",
    "                        blankdict[t.replace(' ','_').lower()] = p.text[l+1:].replace('\\n','').replace('\\xa0','')\n",
    "                    elif 'Coroners Area' in p.text:\n",
    "                        blankdict['coroner_area'] = p.text[13:].strip().replace('\\n','').replace('\\xa0','')\n",
    "                    else:\n",
    "                        print(\"Something we haven't accounted for has happened\")\n",
    "\n",
    "                elif p.text.strip().count(\":\") == 2:\n",
    "                    #This corrects for one odd record in which there are 2 colons but should generalize to fix it for\n",
    "                    #any time this could happen, so long as it happens in the same way\n",
    "                    text_list = p.text.split(':')\n",
    "                    new_string = text_list[0] + text_list[1]\n",
    "                    new_name = re.sub(reg_exp, ' ', new_string).strip()\n",
    "                    blankdict[new_name.replace(' ','_').lower()] = text_list[2].strip().replace('\\n','').replace('\\xa0','')\n",
    "\n",
    "                elif ':' in p.text and p.text.split(':')[0] not in text_cats:\n",
    "                    #Some field names are in the form of \"name_of_decesased\" or \"name_of_coroner\" or are plural/\n",
    "                    #possessive so this smashes those into our preferred naming formats\n",
    "                    if 'Name of' in p.text:\n",
    "                        all_text = p.text.split(':')\n",
    "                        key_name = all_text[0].split(' ')\n",
    "                        blankdict[key_name[2].strip() + '_name'] = all_text[-1].strip()\n",
    "                    else:    \n",
    "                        new_name = re.sub(reg_exp, ' ', p.text)\n",
    "                        text_list = new_name.split(':')\n",
    "                        blankdict[text_list[0].strip().replace(' ','_').lower()] = text_list[1].strip().replace('\\n','').replace('\\xa0','')\n",
    "        blankdict['url'] = record_url\n",
    "        \n",
    "        #A small little check for duplicated ref names\n",
    "        try:\n",
    "            if not blankdict['ref']:\n",
    "                pass\n",
    "            elif blankdict['ref'] in ref_list:\n",
    "                blankdict['ref'] = blankdict['ref'] + 'A'\n",
    "            ref_list.append(blankdict['ref'])\n",
    "        except KeyError:\n",
    "            blankdict['ref'] = ''\n",
    "            \n",
    "        #This appends the final dict to the list\n",
    "        record_text.append(blankdict)\n",
    "        \n",
    "        #this is a seperate process to get the PDF URLs (no matter how many there are) and adds them to their own list   \n",
    "        urls = soup.find_all('li', {'class':'pdf'})\n",
    "        pdf_list = []\n",
    "        for url in urls:\n",
    "            pdf_list.append(url.findNext('a').get('href'))\n",
    "        pdf_urls.append(pdf_list)\n",
    "        \n",
    "        record_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_desc = f\"{str(e)} occurred for {record_url} when trying to work with {p}\"\n",
    "        print(error_desc)\n",
    "        error_catching.append(error_details(record_count, record_url, error_desc))\n",
    "        \n",
    "        #Saving this in case we don't like the error catching.\n",
    "        #import sys\n",
    "        #raise type(e)(str(e) + '\\n' + 'Error for Record: {}, Field: {}'.format(record_url, p)).with_traceback(sys.exc_info()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the third loop to save the PDFs using the deceased Ref as the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Any errors should print out above, but you can also check the error_catching dict\n",
    "#Here we just turn it into a dataframe quickly to easily view\n",
    "\n",
    "error_df = pd.DataFrame(error_catching)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(path_string, name_string):\n",
    "    with open(path_string.format(name_string), 'wb') as d:\n",
    "        d.write(myfile.content)\n",
    "\n",
    "save_path = '/Users/georgiarichards/Desktop/Python/PFDs opioids/All_PDFs8/{}.pdf'\n",
    "\n",
    "potential_names = ['ref', 'deceased_name', 'date_of_report']\n",
    "\n",
    "record_count = 0\n",
    "#This is the final scrape to actually get the URLs and change the name (when possible) to the refs\n",
    "for r_t, p_u in zip(tqdm(record_text), pdf_urls):\n",
    "    if not p_u:\n",
    "        #If there is no pdf at all, we skip it.\n",
    "        continue\n",
    "    else:\n",
    "        #All this does is gets the PDF and downloads it and names it after the reg\n",
    "        #It looks scary and complicated but all it is doing is varying the name in the case of multiple PDFs\n",
    "        #Or naming it for the deceased person if there is no Ref value\n",
    "        #If there is a pdf but no ref or deceased name, this will throw an error and we can adjust.\n",
    "        try:\n",
    "            counter = 0\n",
    "            if len(p_u) > 1:\n",
    "                for p in p_u:\n",
    "                    if counter == 0:\n",
    "                        myfile = get(p)\n",
    "                        named = False\n",
    "                        for x in potential_names:\n",
    "                            try:\n",
    "                                if r_t[x]:\n",
    "                                    save_file(save_path, r_t[x])\n",
    "                                    counter +=1\n",
    "                                    named = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    continue\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                        if not named:       \n",
    "                            save_file(save_path, 'check_record_{}'.format(record_count))\n",
    "                            counter +=1\n",
    "\n",
    "                    else:\n",
    "                        myfile = get(p)\n",
    "                        named = False\n",
    "                        for x in potential_names:\n",
    "                            try:\n",
    "                                if r_t[x]:\n",
    "                                    save_file(save_path, r_t[x] + '_{}'.format(counter))\n",
    "                                    counter +=1\n",
    "                                    named = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    continue\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                        if not named:\n",
    "                            save_file(save_path, 'check_record_{}_{}'.format(record_count, counter))\n",
    "                            counter +=1\n",
    "                                    \n",
    "            else:\n",
    "                myfile = get(p_u[0])\n",
    "                named = False\n",
    "                for x in potential_names:\n",
    "                    try:\n",
    "                        if r_t[x]:\n",
    "                            save_file(save_path, r_t[x])\n",
    "                            named = True\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                if not named:       \n",
    "                    save_file(save_path, 'check_record_{}'.format(record_count))\n",
    "            \n",
    "            record_count += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            import sys\n",
    "            if r_t['ref']:\n",
    "                raise type(e)(str(e) + '\\n' + 'Error for Record: {}'.format(r_t['ref'])).with_traceback(sys.exc_info()[2])\n",
    "            else:\n",
    "                raise type(e)(str(e) + '\\n' + 'Error for Record Number: {}'.format(record_count)).with_traceback(sys.exc_info()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my final step that puts the text data (info on the deceased/case) into a csv file & adds the date it was pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "headers = ['date_of_report', 'date_of_reports', 'ref', 'deceased_name', 'deceased_names', 'coroner_name', 'coroner_area', 'category', 'this_report_is_being_sent_to', 'these_report_are_being_sent_to', 'url']\n",
    "\n",
    "with open('death_info_{}.csv'.format(date.today()), 'w', newline='', encoding='utf-8') as deaths_csv:\n",
    "    writer = csv.DictWriter(deaths_csv, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for record in record_text:\n",
    "        if record == {}:\n",
    "            pass\n",
    "        else:\n",
    "            writer.writerow(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an addition few steps to check what differences there are from the June 2021 records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pdfs7 = os.listdir('All_PDFs7')\n",
    "pdfs8 = os.listdir('All_PDFs8')\n",
    "\n",
    "new_not_old = set(pdfs8).difference(pdfs7)\n",
    "\n",
    "new_not_old_list = list(new_not_old)\n",
    "new_not_old_list.sort()\n",
    "new_not_old_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_not_old_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep21 = pd.read_csv('death_info_2021-09-07.csv')\n",
    "jun21 = pd.read_csv('death_info_2021-06-28.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(jun21.columns)\n",
    "merged = sep21.merge(jun21, on=cols, how='left', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only = merged[merged['_merge'] == 'left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only.to_csv(r'death_info_newsep21.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing for website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_names = pd.read_csv('death_info_2021-09-07.csv')\n",
    "sep_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_names['deceased_name'] = sep_names['deceased_name'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sep_names['deceased_name'] = sep_names['deceased_name'].apply(lambda x: ''.join(i[0] for i in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_names['deceased_name'] = sep_names['deceased_name'].str.replace('\\W', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_names.to_csv('death_info_2021-09-07_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
